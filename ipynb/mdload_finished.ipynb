{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_md_files(md_folder_path):\n",
    "    \"\"\"\n",
    "    Lädt alle .md-Dateien aus einem angegebenen Ordner und gibt sie als Liste von Dokumenten zurück.\n",
    "\n",
    "    :param md_folder_path: Der Pfad zum Ordner, der die .md-Dateien enthält.\n",
    "    :return: Eine Liste mit dem Inhalt aller .md-Dateien.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "\n",
    "    # Durchlaufe alle Dateien im angegebenen Ordner\n",
    "    for filename in os.listdir(md_folder_path):\n",
    "        # Überprüfen, ob die Datei eine .md-Datei ist\n",
    "        if filename.endswith('.md'):\n",
    "            # Vollständigen Pfad zur Datei erstellen\n",
    "            file_path = os.path.join(md_folder_path, filename)\n",
    "            # Datei öffnen und Inhalt lesen\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                # Inhalt zur Dokumentenliste hinzufügen\n",
    "                documents.append(file.read())\n",
    "\n",
    "    return documents\n",
    "\n",
    "# Beispiel: Die Funktion aufrufen\n",
    "md_folder_path = os.path.join('data', 'md')\n",
    "documents = load_md_files(md_folder_path)\n",
    "\n",
    "\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"Document {i}:\\n\")\n",
    "    print(doc)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Verwende ein kleineres Modell wie 't5-small'\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "corrector = pipeline('text2text-generation', model='t5-small', tokenizer='t5-small', device=device)\n",
    "\n",
    "# Funktion, um einen langen Text in Abschnitte zu teilen\n",
    "def split_text(text, max_length=512):\n",
    "    # Teile den Text in Abschnitte von maximaler Länge (hier 512 Tokens)\n",
    "    return [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
    "\n",
    "# Funktion, um den Text zu korrigieren und Abschnitte wieder zusammenzusetzen\n",
    "def correct_long_text(doc, max_length=512):\n",
    "    # Teile den Text in kürzere Abschnitte\n",
    "    sections = split_text(doc, max_length)\n",
    "    corrected_sections = []\n",
    "    \n",
    "    # Korrigiere jeden Abschnitt\n",
    "    for section in sections:\n",
    "        corrected_section = corrector(f\"correct: {section}\", clean_up_tokenization_spaces=True)\n",
    "        corrected_sections.append(corrected_section[0]['generated_text'])\n",
    "    \n",
    "    # Setze die korrigierten Abschnitte wieder zu einem Text zusammen\n",
    "    return ' '.join(corrected_sections)\n",
    "\n",
    "\n",
    "corrected_documents = []\n",
    "\n",
    "# Korrigiere jeden langen Text in der Liste\n",
    "for doc in documents:\n",
    "    corrected_text = correct_long_text(doc)\n",
    "    corrected_documents.append(corrected_text)\n",
    "\n",
    "# Ausgabe der korrigierten Texte\n",
    "for i, (original, corrected) in enumerate(zip(documents, corrected_documents)):\n",
    "    print(f\"Original Dokument {i+1}: {original}\\nKorrigiertes Dokument {i+1}: {corrected}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import language_tool_python\n",
    "\n",
    "# Umgebungsvariable setzen, um die Warnung zu unterdrücken\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Lade das LanguageTool-Modul für Deutsch\n",
    "tool = language_tool_python.LanguageTool('de-DE')\n",
    "\n",
    "# Funktion zur Korrektur eines langen Textes mit LanguageTool\n",
    "def correct_text_with_language_tool(text):\n",
    "    matches = tool.check(text)\n",
    "    corrected_text = language_tool_python.utils.correct(text, matches)\n",
    "    return corrected_text\n",
    "\n",
    "\n",
    "corrected_documents = []\n",
    "\n",
    "# Korrigiere jeden Text in der Liste\n",
    "for doc in documents:\n",
    "    corrected_text = correct_text_with_language_tool(doc)\n",
    "    corrected_documents.append(corrected_text)\n",
    "\n",
    "# Ausgabe der Original- und korrigierten Texte\n",
    "for i, (original, corrected) in enumerate(zip(documents, corrected_documents)):\n",
    "    print(f\"Original Dokument {i+1}:\\n{original}\\n\")\n",
    "    print(f\"Korrigiertes Dokument {i+1}:\\n{corrected}\\n\")\n",
    "    print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "def print_corrections(original, corrected):\n",
    "    display(HTML(f\"\"\"\n",
    "    <div style=\"border: 1px solid black; padding: 10px; margin: 10px; max-height: 300px; overflow-y: auto;\">\n",
    "        <h3>Originaler Text:</h3>\n",
    "        <p style=\"white-space: pre-wrap;\">{original}</p>\n",
    "        <h3>Korrigierter Text:</h3>\n",
    "        <p style=\"white-space: pre-wrap; color: green;\">{corrected}</p>\n",
    "    </div>\n",
    "    \"\"\"))\n",
    "    \n",
    "# Aufruf der Funktion für jedes Dokument\n",
    "for original, corrected in zip(documents, corrected_documents):\n",
    "    print_corrections(original, corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import language_tool_python\n",
    "\n",
    "# Lade das LanguageTool-Modell für Deutsch\n",
    "tool = language_tool_python.LanguageTool('de')\n",
    "\n",
    "# Funktion zur Rechtschreib- und Grammatikprüfung mit LanguageTool\n",
    "def correct_with_languagetool(text):\n",
    "    matches = tool.check(text)\n",
    "    corrected_text = language_tool_python.utils.correct(text, matches)\n",
    "    return corrected_text\n",
    "\n",
    "# Korrigieren von gesplitteten Wörtern\n",
    "def correct_word_splitting(text):\n",
    "    return re.sub(r'\\b(\\w+)\\s*-\\s*(\\w+)\\b', r'\\1\\2', text)\n",
    "\n",
    "# Hauptfunktion zur Bereinigung der Dokumente\n",
    "def clean_documents(documents):\n",
    "    cleaned_documents = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        # Entfernen von problematischen Steuerzeichen (z.B. Steuerzeichen, nicht druckbare Zeichen)\n",
    "        doc = re.sub(r'[\\x00-\\x1F\\x7F]', ' ', doc)  # Entfernt Steuerzeichen\n",
    "        \n",
    "        # Sicherstellen, dass keine doppelten Leerzeichen durch das Entfernen von Zeichen entstehen\n",
    "        doc = re.sub(r'\\s+', ' ', doc).strip()\n",
    "        \n",
    "        # Korrigieren von gesplitteten Wörtern\n",
    "        doc = correct_word_splitting(doc)\n",
    "        \n",
    "        # Textkorrektur mit LanguageTool\n",
    "        doc = correct_with_languagetool(doc)\n",
    "        \n",
    "        # Bereinigtes Dokument zur Liste hinzufügen\n",
    "        cleaned_documents.append(doc)\n",
    "    \n",
    "    return cleaned_documents\n",
    "\n",
    "cleaned_documents = clean_documents(documents)\n",
    "\n",
    "    \n",
    "for i, doc in enumerate(cleaned_documents):\n",
    "    print(f\"cleaned_documents {i}:\\n\")\n",
    "    print(doc)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(documents):\n",
    "    # Erlaubte Zeichen: Buchstaben (inkl. ä, ö, ü, ß), Ziffern, Bindestrich, Punkt, Komma, Ausrufezeichen und Fragezeichen\n",
    "    pattern = r'[^a-zA-ZäöüÄÖÜß0-9\\s.,!?-]'\n",
    "    \n",
    "    # Entfernt alle unerwünschten Zeichen aus jedem Dokument\n",
    "    cleaned_documents = [re.sub(pattern, '', doc) for doc in documents]\n",
    "    return cleaned_documents\n",
    "\n",
    "\n",
    "# Bereinigte Dokumente\n",
    "cleaned_documents_2 = clean_text(cleaned_documents)\n",
    "\n",
    "# Ausgabe der bereinigten Dokumente\n",
    "for doc in cleaned_documents_2:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def chunk_documents(documents, chunk_size=8, overlap=2):\n",
    "    \"\"\"\n",
    "    Segmentiert eine Liste von Texten in Sätze und erstellt Chunks mit Overlap.\n",
    "    \n",
    "    :param documents: Liste von Texten (Strings), die segmentiert werden sollen\n",
    "    :param chunk_size: Anzahl der Sätze pro Chunk (z.B. 8 Sätze pro Chunk)\n",
    "    :param overlap: Anzahl der Sätze, die zwischen aufeinanderfolgenden Chunks überlappen sollen (z.B. 2 Sätze)\n",
    "    :return: Liste von Chunks, wobei jeder Chunk als separates Dokument gespeichert wird\n",
    "    \"\"\"\n",
    "    # Lade das deutsche Modell\n",
    "    nlp = spacy.load('de_core_news_sm')\n",
    "    \n",
    "    # Liste zum Speichern der Chunks als separate Dokumente\n",
    "    chunked_documents = []\n",
    "    \n",
    "    # Verarbeitung und Segmentierung jedes Dokuments in Sätze\n",
    "    for text in documents:\n",
    "        doc = nlp(text)\n",
    "        sentences = list(doc.sents)\n",
    "        \n",
    "        # Chunks erstellen und als eigene Dokumente speichern\n",
    "        for i in range(0, len(sentences), chunk_size - overlap):\n",
    "            chunk = sentences[i:i + chunk_size]\n",
    "            chunked_documents.append(\" \".join([sent.text for sent in chunk]))\n",
    "    \n",
    "    return chunked_documents\n",
    "\n",
    "chunked_documents = chunk_documents(cleaned_documents_2, chunk_size=8, overlap=2)\n",
    "\n",
    "   # Ausgabe aller Chunks\n",
    "print(\"Alle Chunks:\")\n",
    "for i, chunk in enumerate(chunked_documents, 1):\n",
    "    print(f\"Chunk {i}: {chunk}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "def extract_keywords_from_chunk(chunk, top_n=5):\n",
    "    \"\"\"\n",
    "    Extrahiert die häufigsten Substantive und Adjektive aus einem Textchunk.\n",
    "    \n",
    "    :param chunk: Ein Textchunk als String\n",
    "    :param top_n: Anzahl der Top-Keywords, die extrahiert werden sollen\n",
    "    :return: Liste der Top-Keywords\n",
    "    \"\"\"\n",
    "    nlp = spacy.load('de_core_news_sm')\n",
    "    doc = nlp(chunk)\n",
    "    \n",
    "    # Sammeln von Substantiven und Adjektiven\n",
    "    keywords = [token.text for token in doc if token.pos_ in ['NOUN', 'ADJ']]\n",
    "    \n",
    "    # Häufigkeit der Keywords zählen\n",
    "    keyword_freq = Counter(keywords)\n",
    "    \n",
    "    # Die häufigsten Keywords extrahieren\n",
    "    most_common_keywords = [keyword for keyword, freq in keyword_freq.most_common(top_n)]\n",
    "    \n",
    "    return most_common_keywords\n",
    "\n",
    "def extract_keywords_from_chunks(chunked_documents, top_n=5):\n",
    "    \"\"\"\n",
    "    Extrahiert die relevantesten Keywords aus jeder Chunk in den gechunkten Dokumenten.\n",
    "    \n",
    "    :param chunked_documents: Liste von gechunkten Dokumenten\n",
    "    :param top_n: Anzahl der Top-Keywords pro Chunk\n",
    "    :return: Liste von Listen, die Keywords für jeden Chunk enthalten\n",
    "    \"\"\"\n",
    "    all_keywords = []\n",
    "    \n",
    "    for chunk in chunked_documents:\n",
    "        keywords = extract_keywords_from_chunk(chunk, top_n)\n",
    "        all_keywords.append(keywords)\n",
    "    \n",
    "    return all_keywords\n",
    "\n",
    "# Beispiel für die Verwendung\n",
    "keywords_per_chunk = extract_keywords_from_chunks(chunked_documents, top_n=5)\n",
    "\n",
    "# Ausgabe der Keywords pro Chunk\n",
    "print(\"Keywords pro Chunk:\")\n",
    "for i, keywords in enumerate(keywords_per_chunk, 1):\n",
    "    print(f\"Chunk {i} Keywords: {keywords}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_wordcloud(keywords_per_chunk):\n",
    "    \"\"\"\n",
    "    Erstellt eine Wordcloud basierend auf den extrahierten Keywords aus den Chunks.\n",
    "    \n",
    "    :param keywords_per_chunk: Liste von Listen, die Keywords für jeden Chunk enthalten\n",
    "    \"\"\"\n",
    "    # Alle Keywords zu einem einzigen String zusammenfügen\n",
    "    all_keywords = ' '.join([' '.join(keywords) for keywords in keywords_per_chunk])\n",
    "    \n",
    "    # WordCloud generieren\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_keywords)\n",
    "    \n",
    "    # WordCloud anzeigen\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Beispiel für die Verwendung\n",
    "generate_wordcloud(keywords_per_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def save_processed_documents(processed_documents, filename):\n",
    "    # Sicherstellen, dass der Ordner \"data/pickle\" existiert\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    \n",
    "    # Speichern der Datei im angegebenen Pfad\n",
    "    with open(filename, \"wb\") as file:\n",
    "        pickle.dump(processed_documents, file)\n",
    "    \n",
    "    print(f\"Die Datei '{filename}' wurde erfolgreich gespeichert.\")\n",
    "\n",
    "# Beispielnutzung:\n",
    "save_processed_documents(chunked_documents, filename=\"data/pickle/chunked_doc_list.pkl\")\n",
    "save_processed_documents(keywords_per_chunk, filename=\"data/pickle/chunk_keywords.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predictiv_chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
