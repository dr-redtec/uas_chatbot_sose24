{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'document_embeddings_v9' erfolgreich gelöscht.\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings\n",
    "\n",
    "def delete_chroma_collection(collection_name='document_embeddings_v9', persist_directory='chroma_db_v4'):\n",
    "    # Öffne die Chroma-Datenbank mit Persistierung\n",
    "    client = chromadb.PersistentClient(\n",
    "        path=persist_directory,\n",
    "        settings=Settings(),\n",
    "        tenant=DEFAULT_TENANT,\n",
    "        database=DEFAULT_DATABASE,\n",
    "    )\n",
    "    \n",
    "    # Überprüfe, ob die Collection existiert, und lösche sie, wenn ja\n",
    "    existing_collections = [col.name for col in client.list_collections()]\n",
    "    if collection_name in existing_collections:\n",
    "        client.delete_collection(name=collection_name)\n",
    "        print(f\"Collection '{collection_name}' erfolgreich gelöscht.\")\n",
    "    else:\n",
    "        print(f\"Collection '{collection_name}' nicht gefunden.\")\n",
    "        \n",
    "delete_chroma_collection(collection_name='document_embeddings_v9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erste 5 Elemente von chunked_documents:\n",
      "Element 1: Der blaue Altglascontainer das kommt hinein: • Saubere Flaschen und Deckelgläser nach den Farben Weiß, Grün und Braun getrennt; blaues oder buntes Glas bitte immer zum Grünglas geben. • Glasflaschen • Konservengläser das kommt nicht hinein: • Flachglas wie Spiegel und Scheiben (Restmüll oder Sperrmüll) • Trinkgläser und Blumenvasen (Restmüll)\n",
      "Element 2: • Trinkgläser und Blumenvasen (Restmüll) • Porzellan und Keramik (Restmüll) • Glühbirnen (Restmüll), Energiesparlampen (Schadstoffmobil oder Wertstoffhöfe) • Stark verschmutzte Gläser (Restmüll) FES\n",
      "Element 3: • Stark verschmutzte Gläser (Restmüll) FES Frankfurter Entsorgungsund Service GmbH Weidenbornstraße 40 60389 Frankfurt am Main Servicetelefon: 0800 20080070 Servicetelefax: 069 21231323 services@fesfrankfurt.de www.fesfrankfurt.de FESServicecenter Liebfrauenberg 52–54 (Ecke Töngesgasse/Schärfengässchen) 60313 Frankfurt am Main Mo. – Fr. 10.00 – 18.00 Uhr Sa. 10.00 – 16.00 Uhr was kommt, wo hinein? Abfall richtig trennen – Wertstoffe sichern. Richtig trennen leicht gemacht:\n",
      "Element 4: Abfall richtig trennen – Wertstoffe sichern. Richtig trennen leicht gemacht: Zu jedem Haus in Frankfurt gehören 4 Abfallbehälter: 3 Tonnen, deren Inhalt als wertvoller Rohstoff zu neuen Produkten weiterverarbeitet wird, und die graue Restmülltonne: • Die gelbe Tonne für Verpackungen • Die grüne Tonne für Papier und Pappe • Die braune Tonne für Bioabfälle\n",
      "Element 5: Die grüne Tonne für Papier und Pappe • Die braune Tonne für Bioabfälle • Die graue Tonne für den restlichen Abfall: den Restmüll darüber hinaus gibt es folgende Entsorgungsangebote: Altglas: Altglascontainer für Flaschen und Deckelgläser finden Sie an öffentlichen Plätzen. Altkleider: Container für Altkleider und Schuhe finden Sie in jedem Stadtteil auf öffentlichen Plätzen sowie auf den FESWertstoffhöfen. Schadstoffe: Abfälle aus oder mit gefährlichen Stoffen wie Farben, Spraydosen, Lösemitteln etc. können Sie am FESSchadstoffmobil abgeben: Alle Standorte und Termine finden Sie auf www.fesfrankfurt.de oder im FESMagazin OSKAR, das dreimal im Jahr an alle Haushalte verteilt wird. Sperrmüllabholung: Bis zu 10 m³ holt die FES kostenlos bei Ihnen ab.\n",
      "\n",
      "Erste 5 Elemente von keywords_per_chunk:\n",
      "Element 1: ['blaue', 'Altglascontainer', 'Saubere', 'Flaschen', 'Deckelgläser']\n",
      "Element 2: ['Restmüll', '•', 'Trinkgläser', 'Blumenvasen', 'Porzellan']\n",
      "Element 3: ['Uhr', 'verschmutzte', 'Gläser', 'Restmüll', 'Frankfurter']\n",
      "Element 4: ['Tonne', 'Abfall', 'Wertstoffe', 'Haus', 'Abfallbehälter']\n",
      "Element 5: ['Tonne', 'öffentlichen', 'Plätzen', 'Altkleider', 'grüne']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def load_pickle_files():\n",
    "    # Pfade zu den Pickle-Dateien\n",
    "    chunked_doc_path = \"data/pickle/chunked_doc_list.pkl\"\n",
    "    keywords_path = \"data/pickle/chunk_keywords.pkl\"\n",
    "    \n",
    "    # Laden der Pickle-Dateien\n",
    "    with open(chunked_doc_path, 'rb') as doc_file:\n",
    "        chunked_documents = pickle.load(doc_file)\n",
    "    \n",
    "    with open(keywords_path, 'rb') as keywords_file:\n",
    "        keywords_per_chunk = pickle.load(keywords_file)\n",
    "    \n",
    "    return chunked_documents, keywords_per_chunk\n",
    "\n",
    "def print_first_five_elements(list_obj):\n",
    "    # Überprüfen, ob die Liste mindestens 5 Elemente hat\n",
    "    for i, element in enumerate(list_obj[:5]):\n",
    "        print(f\"Element {i+1}: {element}\")\n",
    "\n",
    "# Benutzen der Funktionen\n",
    "chunked_documents, keywords_per_chunk = load_pickle_files()\n",
    "\n",
    "# Ausgabe der ersten 5 Elemente der beiden Listen\n",
    "print(\"Erste 5 Elemente von chunked_documents:\")\n",
    "print_first_five_elements(chunked_documents)\n",
    "\n",
    "print(\"\\nErste 5 Elemente von keywords_per_chunk:\")\n",
    "print_first_five_elements(keywords_per_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/data1/Programming/predictiv_chatbot/predictiv_chatbot/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma-Datenbank 'document_embeddings_v9' erfolgreich erstellt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/data1/Programming/predictiv_chatbot/predictiv_chatbot/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding erstellt\n",
      "Embedding erstellt\n",
      "Embedding erstellt\n",
      "Embedding erstellt\n",
      "Embedding erstellt\n",
      "Embedding erstellt\n",
      "Embedding erstellt\n",
      "Embedding erstellt\n",
      "Embedding erstellt\n",
      "Embedding erstellt\n",
      "Embedding erstellt\n",
      "Embedding erstellt\n",
      "Embedding erstellt\n",
      "Embedding erstellt\n",
      "Embedding erstellt\n",
      "Embedding erstellt\n",
      "Embedding erstellt\n",
      "Embedding erstellt\n",
      "Embedding erstellt\n",
      "Embedding erstellt\n",
      "Embedding erstellt\n",
      "Embedding erstellt\n",
      "Embedding erstellt\n",
      "Embedding erstellt\n",
      "Embedding erstellt\n",
      "Alle Chunks wurden erfolgreich mit ihren Embeddings und Metadaten zu Chroma hinzugefügt.\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings\n",
    "#from chromadb.utils import embedding_functions\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def create_embeddings(chunk):\n",
    "    \n",
    "    # Lade ein vortrainiertes Modell\n",
    "    model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "    # Erstelle Embeddings für die Texte\n",
    "    chunk_embedding = model.encode(chunk)\n",
    "\n",
    "    print(\"Embedding erstellt\")\n",
    "    return chunk_embedding\n",
    "\n",
    "def create_chroma_v2(chunk, keywords, collection_name, persist_directory):\n",
    "    # Erstelle oder öffne eine Chroma-Datenbank mit Persistierung\n",
    "    client = chromadb.PersistentClient(\n",
    "        path=persist_directory,\n",
    "        settings=Settings(),\n",
    "        tenant=DEFAULT_TENANT,\n",
    "        database=DEFAULT_DATABASE,\n",
    "    )\n",
    "    \n",
    "    # Erstelle eine neue Collection in der Datenbank oder lade eine bestehende\n",
    "    collection = client.get_or_create_collection(name=collection_name)\n",
    "    \n",
    "    print(f\"Chroma-Datenbank '{collection_name}' erfolgreich erstellt.\")\n",
    "\n",
    "    \n",
    "    #chunk_embedding = create_embeddings(chunks)\n",
    "    \n",
    "    #collection.add(\n",
    "    #embeddings = chunk_embedding.tolist(),  # Konvertiere Embeddings zu einer Liste von Listen\n",
    "    #documents = [chunks],\n",
    "    #metadatas = [{\"keywords\": \"student info\"}],\n",
    "    #ids = [\"id1\"]\n",
    "    #)\n",
    "    \n",
    "    # Angenommen, du hast bereits `chunked_documents` und `keywords_per_chunk` geladen\n",
    "# und die Funktion `create_embeddings(chunks)` ist implementiert\n",
    "\n",
    "    for i, (chunk, keywords) in enumerate(zip(chunked_documents, keywords_per_chunk)):\n",
    "        # IDs werden dynamisch erzeugt, z.B. basierend auf dem Index\n",
    "        doc_id = f\"id_{i+1}\"\n",
    "    \n",
    "        # Erzeuge Embeddings für den aktuellen Chunk\n",
    "        chunk_embedding = create_embeddings(chunk)\n",
    "\n",
    "        # Konvertiere die Embeddings in eine Liste von Listen\n",
    "        embeddings = chunk_embedding.tolist()\n",
    "    \n",
    "        # Konvertiere die Liste von Keywords in einen String\n",
    "        keywords_str = \", \".join(keywords)  # Keywords in einen einzigen String umwandeln\n",
    "    \n",
    "        # Hinzufügen des Chunks mit den entsprechenden Keywords und Embeddings\n",
    "        collection.add(\n",
    "            documents=[chunk],  # Der aktuelle Chunk\n",
    "            metadatas=[{\"keywords\": keywords_str}],  # Keywords als String in den Metadaten\n",
    "            ids=[doc_id],  # Eine eindeutige ID für jeden Eintrag\n",
    "            embeddings=embeddings  # Die Embeddings als Liste von Listen\n",
    "        )\n",
    "\n",
    "    print(\"Alle Chunks wurden erfolgreich mit ihren Embeddings und Metadaten zu Chroma hinzugefügt.\")\n",
    "    \n",
    "    \n",
    "    # results = collection.query(\n",
    "    # query_texts=[\"Was kommt in den gelbe Tonne?\"],\n",
    "    # n_results=2\n",
    "    #)\n",
    "    # print(results)\n",
    "    # print(collection.get(include=['embeddings', 'documents', 'metadatas']))\n",
    "\n",
    "\n",
    "    \n",
    "collection_name='document_embeddings_v9'\n",
    "persist_directory='chroma_db_v4'\n",
    "test = \"\"\"\n",
    "Alexandra Thompson, a 19-year-old computer science sophomore with a 3.7 GPA,\n",
    "is a member of the programming and chess clubs who enjoys pizza, swimming, and hiking\n",
    "in her free time in hopes of working at a tech company after graduating from the University of Washington.\n",
    "\"\"\"\n",
    "\n",
    "create_chroma_v2(chunked_documents, keywords_per_chunk, collection_name, persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ergebnis 1:\n",
      "Dokument: • Stark verschmutzte FastFood-Verpackungen wie Pizzakartons (Restmüll) Hinweis: Ist zu viel Restmüll in der gelben Tonne, können die Wertstoffe nicht mehr aussortiert und weiterverarbeitet werden. Standort: Am Wohnhaus Leerung: 14täglich die grüne Altpapiertonne das kommt hinein: • In die grüne Altpapiertonne gehören Papier und Pappe – ohne Beschichtung und unverschmutzt. • Pappen und Kartonagen • Zeitungen, Illustrierte, Kataloge • Bücher und Hefte das kommt nicht hinein: • Stark verschmutztes Papier (Restmüll) •\n",
      "Keywords: grüne, Altpapiertonne, Papier, FastFood-Verpackungen, Hinweis\n",
      "Score: 16.188478962787908\n",
      "\n",
      "Dokument: Verpackungen aus Metall wie Konservenund Getränkedosen • Geschäumte Kunststoffe, z. B. Obstund Gemüseverpackungen das kommt nicht hinein: • Verpackungen aus Papier und Pappe (Altpapiertonne) • Verpackungen aus Glas (Altglascontainer) • Stark verschmutzte FastFood-Verpackungen wie Pizzakartons (Restmüll) Hinweis: Ist zu viel Restmüll in der gelben Tonne, können die Wertstoffe nicht mehr aussortiert und weiterverarbeitet werden. Standort:\n",
      "Keywords: Verpackungen, Metall, Konservenund, Getränkedosen, •\n",
      "Score: 18.667627733772157\n",
      "\n",
      "Dokument: Pappen und Kartonagen • Zeitungen, Illustrierte, Kataloge • Bücher und Hefte das kommt nicht hinein: • Stark verschmutztes Papier (Restmüll) • Beschichtetes Papier (Restmüll) • Pergamentund Hygienepapiere, Windeln (Restmüll) Hinweis: Näheres über sichere Aktenvernichtung erfahren Sie unter der Servicetelefonnummer 0800 200800770. Standort: Am Wohnhaus Leerung: 14täglich die braune Biotonne das kommt hinein: •\n",
      "Keywords: •, Papier, Pappen, Kartonagen, Zeitungen\n",
      "Score: 19.12259541570001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Verbindungsparameter\n",
    "persist_directory = 'chroma_db_v4'  # Pfad zum Speicherverzeichnis\n",
    "DEFAULT_TENANT = \"default_tenant\"  # Name des Mandanten\n",
    "DEFAULT_DATABASE = \"default_database\"  # Name der Datenbank\n",
    "\n",
    "# Chroma Client erstellen\n",
    "client = chromadb.PersistentClient(\n",
    "    path=persist_directory,\n",
    "    settings=chromadb.Settings(),\n",
    "    tenant=DEFAULT_TENANT,\n",
    "    database=DEFAULT_DATABASE,\n",
    ")\n",
    "\n",
    "# Text-zu-Vektor Modell laden (z.B. SentenceTransformer)\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')  # oder ein anderes Modell\n",
    "\n",
    "# Text in Vektor umwandeln\n",
    "def text_to_vector(text):\n",
    "    return model.encode(text).tolist()  # Umwandlung in Liste\n",
    "\n",
    "# Textabfrage durchführen\n",
    "def query_chroma_with_text(client, collection_name, query_text, n_results=3):\n",
    "    query_vector = text_to_vector(query_text)\n",
    "    collection = client.get_collection(collection_name)\n",
    "    # Abfrage mit Begrenzung auf die besten n Ergebnisse\n",
    "    results = collection.query(query_embeddings=[query_vector], n_results=n_results)\n",
    "    return results\n",
    "\n",
    "# Beispielhafte Verwendung\n",
    "query_text = \"Was kommt in die gelbe Tonne?\"  # Ihre Textabfrage\n",
    "collection_name = 'document_embeddings_v9'  # Name der zu durchsuchenden Sammlung\n",
    "\n",
    "# Abfrageergebnisse abrufen\n",
    "results = query_chroma_with_text(client, collection_name, query_text)\n",
    "\n",
    "# Nur die besten 3 Ergebnisse ausgeben (docs, keywords und score)\n",
    "for i in range(len(results['documents'])):\n",
    "    print(f\"Ergebnis {i + 1}:\")\n",
    "    # Jedes Dokument, die zugehörigen Keywords und den Score anzeigen\n",
    "    for doc, metadata, score in zip(results['documents'][i], results['metadatas'][i], results['distances'][i]):\n",
    "        print(\"Dokument:\", doc)\n",
    "        print(\"Keywords:\", metadata.get('keywords', 'Keine Keywords verfügbar'))\n",
    "        print(\"Score:\", score)\n",
    "        print()  # Leerzeile zur Trennung der Ergebnisse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Alternatives Modell, das öffentlich verfügbar ist\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# Erstelle Embeddings für die Texte\n",
    "chunks = [\"Dein erster Textchunk\", \"Dein zweiter Textchunk\"]  # Beispieltexte\n",
    "students_embeddings = model.encode(chunks)\n",
    "\n",
    "print(students_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predictiv_chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
