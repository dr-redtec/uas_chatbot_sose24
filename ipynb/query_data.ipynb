{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Chunks:\n",
      "Chunk 1:\n",
      "Score: 11476.817293890093\n",
      "Content: Zeitungen, Illustrierte, Kataloge\n",
      " Bücher und Hefte\n",
      "Das kommt nicht hinein\n",
      " Stark verschmutztes Papier (Restmüll)\n",
      " Beschichtetes Papier (Restmüll)\n",
      " Pergament- und Hygienepapiere, Windeln (Restmüll)\n",
      "Metadata: {'page': 1, 'source': 'data/was_kommt_wo_hinein_de.pdf', 'start_index': 1865}\n",
      "\n",
      "---\n",
      "\n",
      "Chunk 2:\n",
      "Score: 12260.387975519536\n",
      "Content: Beschichtetes Papier (Restmüll)\n",
      " Pergament- und Hygienepapiere, Windeln (Restmüll)\n",
      "Näheres über sichere Aktenvernichtung erfahren Sie unter der Servicetelefonnummer 0800 2008007-70.Das kommt hinein\n",
      "Alle biologisch abbaubaren Abfälle aus privaten Haushalten.\n",
      "Metadata: {'page': 1, 'source': 'data/was_kommt_wo_hinein_de.pdf', 'start_index': 1983}\n",
      "\n",
      "---\n",
      "\n",
      "Chunk 3:\n",
      "Score: 12444.317039466961\n",
      "Content: Das kommt nicht hinein\n",
      " Plastiktüten, Verpackungen aus Kunststoff und Metall \n",
      " (gelbe Verpackungstonne)\n",
      " Kehricht und Staubsaugerbeutel (Restmüll)\n",
      " Zigarettenkippen (Restmüll)\n",
      " Windeln und andere gebrauchte Hygieneartikel (Restmüll)\n",
      "Metadata: {'page': 1, 'source': 'data/was_kommt_wo_hinein_de.pdf', 'start_index': 2614}\n",
      "\n",
      "---\n",
      "\n",
      "Chunk 4:\n",
      "Score: 12616.373169438728\n",
      "Content: Schadstoffe \n",
      "Abfälle aus oder mit gefährlichen Stoffen wie Farben, Spraydosen, Löse-mitteln etc. können Sie am FES-Schadstoffmobil abgeben: Alle Standorte \n",
      "und Termine ﬁ  nden Sie auf www.fes-frankfurt.de oder im FES-Magazin OSKAR, das dreimal im Jahr an alle Haushalte verteilt wird.\n",
      "Metadata: {'page': 0, 'source': 'data/was_kommt_wo_hinein_de.pdf', 'start_index': 1838}\n",
      "\n",
      "---\n",
      "\n",
      "Chunk 5:\n",
      "Score: 12848.241179148683\n",
      "Content: Das kommt nicht hinein\n",
      " Flachglas wie Spiegel und Scheiben (Restmüll oder Sperrmüll)\n",
      " Trinkgläser und Blumenvasen (Restmüll)\n",
      " Porzellan und Keramik (Restmüll)\n",
      " Glühbirnen (Restmüll), Energiesparlampen \n",
      " (Schadstoffmobil oder Wertstoffhöfe)\n",
      " Stark verschmutzte Gläser (Restmüll)\n",
      "Metadata: {'page': 0, 'source': 'data/was_kommt_wo_hinein_de.pdf', 'start_index': 832}\n",
      "\n",
      "---\n",
      "\n",
      "Response: In die gelbe Verpackungstonne, aber nur wenn es sich um Plastiktüten, Verpackungen aus Kunststoff und Metall handelt. Wenn es sich jedoch um biologisch abbaubare Abfälle wie Windeln oder gebrauchte Hygieneartikel handelt, dann kommt der Bio-Müll in die grüne Tonne (nicht explizit erwähnt, aber anhand des Kontexts schlussfolgern).\n",
      "Sources: ['Unknown Source', 'Unknown Source', 'Unknown Source', 'Unknown Source', 'Unknown Source']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'In die gelbe Verpackungstonne, aber nur wenn es sich um Plastiktüten, Verpackungen aus Kunststoff und Metall handelt. Wenn es sich jedoch um biologisch abbaubare Abfälle wie Windeln oder gebrauchte Hygieneartikel handelt, dann kommt der Bio-Müll in die grüne Tonne (nicht explizit erwähnt, aber anhand des Kontexts schlussfolgern).'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "\n",
    "CHROMA_PATH = \"chroma\"\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\"\n",
    "\n",
    "def get_embedding_function():\n",
    "    # Verwende OllamaEmbeddings mit deinem lokalen Ollama-Server und dem Modell \"llama3.1:8b\"\n",
    "    embeddings = OllamaEmbeddings(\n",
    "        model=\"llama3.1:8b\",  # Modellname, wie auf dem Server konfiguriert\n",
    "        base_url=\"http://192.168.180.131:11434\"  # IP-Adresse und Port deines Servers\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "def query_rag(query_text: str):\n",
    "    # Prepare the DB.\n",
    "    embedding_function = get_embedding_function()\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "\n",
    "    # Search the DB.\n",
    "    results = db.similarity_search_with_score(query_text, k=5)\n",
    "\n",
    "    # Print out the chunks retrieved from the Chroma database\n",
    "    print(\"Retrieved Chunks:\")\n",
    "    for i, (doc, score) in enumerate(results):\n",
    "        print(f\"Chunk {i+1}:\")\n",
    "        print(f\"Score: {score}\")\n",
    "        print(f\"Content: {doc.page_content}\")\n",
    "        print(f\"Metadata: {doc.metadata}\")\n",
    "        print(\"\\n---\\n\")\n",
    "\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "\n",
    "    # Use local Ollama model with llama3.1 8b\n",
    "    model = Ollama(\n",
    "        model=\"llama3.1:8b\", \n",
    "        base_url=\"http://192.168.180.131:11434\"  # IP-Adresse und Port deines Servers\n",
    "    )\n",
    "    response_text = model.invoke(prompt)\n",
    "\n",
    "    # Collect sources\n",
    "    sources = []\n",
    "    for doc, _score in results:\n",
    "        source_id = doc.metadata.get(\"id\", \"Unknown Source\")  # Verwenden Sie 'Unknown Source', wenn 'id' fehlt\n",
    "        sources.append(source_id)\n",
    "\n",
    "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "    print(formatted_response)\n",
    "    return response_text\n",
    "\n",
    "# Direkt im Notebook ausführen\n",
    "query_text = \"In welche Tonne kommt Bio Müll?\"  # Setzen Sie hier Ihren Abfragetext ein\n",
    "query_rag(query_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predictiv_chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
