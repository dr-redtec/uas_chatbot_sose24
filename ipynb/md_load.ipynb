{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_md_files(md_folder_path):\n",
    "    \"\"\"\n",
    "    Lädt alle .md-Dateien aus einem angegebenen Ordner und gibt sie als Liste von Dokumenten zurück.\n",
    "\n",
    "    :param md_folder_path: Der Pfad zum Ordner, der die .md-Dateien enthält.\n",
    "    :return: Eine Liste mit dem Inhalt aller .md-Dateien.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "\n",
    "    # Durchlaufe alle Dateien im angegebenen Ordner\n",
    "    for filename in os.listdir(md_folder_path):\n",
    "        # Überprüfen, ob die Datei eine .md-Datei ist\n",
    "        if filename.endswith('.md'):\n",
    "            # Vollständigen Pfad zur Datei erstellen\n",
    "            file_path = os.path.join(md_folder_path, filename)\n",
    "            # Datei öffnen und Inhalt lesen\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                # Inhalt zur Dokumentenliste hinzufügen\n",
    "                documents.append(file.read())\n",
    "\n",
    "    return documents\n",
    "\n",
    "# Beispiel: Die Funktion aufrufen\n",
    "md_folder_path = os.path.join('data', 'md')\n",
    "documents = load_md_files(md_folder_path)\n",
    "\n",
    "\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"Document {i}:\\n\")\n",
    "    print(doc)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import language_tool_python\n",
    "\n",
    "# Lade das LanguageTool-Modell für Deutsch\n",
    "tool = language_tool_python.LanguageTool('de')\n",
    "\n",
    "# Funktion zur Rechtschreib- und Grammatikprüfung mit LanguageTool\n",
    "def correct_with_languagetool(text):\n",
    "    matches = tool.check(text)\n",
    "    corrected_text = language_tool_python.utils.correct(text, matches)\n",
    "    return corrected_text\n",
    "\n",
    "# Korrigieren von gesplitteten Wörtern\n",
    "def correct_word_splitting(text):\n",
    "    return re.sub(r'\\b(\\w+)\\s*-\\s*(\\w+)\\b', r'\\1\\2', text)\n",
    "\n",
    "# Hauptfunktion zur Bereinigung der Dokumente\n",
    "def clean_documents(documents):\n",
    "    cleaned_documents = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        # Entfernen von problematischen Steuerzeichen (z.B. Steuerzeichen, nicht druckbare Zeichen)\n",
    "        doc = re.sub(r'[\\x00-\\x1F\\x7F]', ' ', doc)  # Entfernt Steuerzeichen\n",
    "        \n",
    "        # Sicherstellen, dass keine doppelten Leerzeichen durch das Entfernen von Zeichen entstehen\n",
    "        doc = re.sub(r'\\s+', ' ', doc).strip()\n",
    "        \n",
    "        # Korrigieren von gesplitteten Wörtern\n",
    "        doc = correct_word_splitting(doc)\n",
    "        \n",
    "        # Textkorrektur mit LanguageTool\n",
    "        doc = correct_with_languagetool(doc)\n",
    "        \n",
    "        # Bereinigtes Dokument zur Liste hinzufügen\n",
    "        cleaned_documents.append(doc)\n",
    "    \n",
    "    return cleaned_documents\n",
    "\n",
    "cleaned_documents = clean_documents(documents)\n",
    "\n",
    "    \n",
    "for i, doc in enumerate(cleaned_documents):\n",
    "    print(f\"cleaned_documents {i}:\\n\")\n",
    "    print(doc)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(documents):\n",
    "    # Erlaubte Zeichen: Buchstaben (inkl. ä, ö, ü, ß), Ziffern, Bindestrich, Punkt, Komma, Ausrufezeichen und Fragezeichen\n",
    "    pattern = r'[^a-zA-ZäöüÄÖÜß0-9\\s.,!?-]'\n",
    "    \n",
    "    # Entfernt alle unerwünschten Zeichen aus jedem Dokument\n",
    "    cleaned_documents = [re.sub(pattern, '', doc) for doc in documents]\n",
    "    return cleaned_documents\n",
    "\n",
    "\n",
    "# Bereinigte Dokumente\n",
    "cleaned_documents_2 = clean_text(cleaned_documents)\n",
    "\n",
    "# Ausgabe der bereinigten Dokumente\n",
    "for doc in cleaned_documents_2:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def chunk_documents(documents):\n",
    "    \"\"\"\n",
    "    Segmentiert eine Liste von Texten in Sätze.\n",
    "    \n",
    "    :param documents: Liste von Texten (Strings), die segmentiert werden sollen\n",
    "    :return: Liste von Listen, wobei jede innere Liste die Sätze eines Dokuments enthält\n",
    "    \"\"\"\n",
    "    # Lade das deutsche Modell\n",
    "    nlp = spacy.load('de_core_news_sm')\n",
    "    \n",
    "    # Liste zum Speichern der segmentierten Dokumente\n",
    "    chunked_documents = []\n",
    "    \n",
    "    # Verarbeitung und Segmentierung jedes Dokuments in Sätze\n",
    "    for text in documents:\n",
    "        doc = nlp(text)\n",
    "        sentences = list(doc.sents)\n",
    "        chunked_documents.append(sentences)\n",
    "    \n",
    "    return chunked_documents\n",
    "\n",
    "# Aufruf der Funktion\n",
    "result = chunk_documents(cleaned_documents_2)\n",
    "\n",
    "# Ausgabe der Resultate\n",
    "for i, doc in enumerate(result):\n",
    "    print(f\"Dokument {i+1}:\")\n",
    "    for sentence in doc:\n",
    "        print(sentence)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgabe der Chunks\n",
    "for i, doc in enumerate(result):\n",
    "    print(f\"Chunks für Dokument {i+1}:\")\n",
    "    for j, sentence in enumerate(doc):\n",
    "        print(f\"  Chunk {j+1}: {sentence}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def adjacent_sentence_clustering(documents, similarity_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Cluster adjacent sentences in the documents based on cosine similarity.\n",
    "\n",
    "    Args:\n",
    "    documents (list of str): List of documents (each document is a string of sentences).\n",
    "    similarity_threshold (float): Threshold to determine if sentences should be in the same cluster.\n",
    "\n",
    "    Returns:\n",
    "    list of list of str: List of clusters (chunks) for each document.\n",
    "    \"\"\"\n",
    "    clusters = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        sentences = doc.split('. ')\n",
    "        if len(sentences) == 1:  # Falls das Dokument nur einen Satz enthält\n",
    "            clusters.append([doc])\n",
    "            continue\n",
    "        \n",
    "        vectorizer = TfidfVectorizer().fit_transform(sentences)\n",
    "        vectors = vectorizer.toarray()\n",
    "        \n",
    "        clusters_in_doc = []\n",
    "        current_cluster = [sentences[0]]\n",
    "        \n",
    "        for j in range(1, len(sentences)):\n",
    "            similarity = cosine_similarity(vectors[j - 1].reshape(1, -1), vectors[j].reshape(1, -1))[0][0]\n",
    "            \n",
    "            if similarity >= similarity_threshold:\n",
    "                current_cluster.append(sentences[j])\n",
    "            else:\n",
    "                clusters_in_doc.append(' '.join(current_cluster))\n",
    "                current_cluster = [sentences[j]]\n",
    "        \n",
    "        # Add the last cluster\n",
    "        if current_cluster:\n",
    "            clusters_in_doc.append(' '.join(current_cluster))\n",
    "        \n",
    "        clusters.append(clusters_in_doc)\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "\n",
    "chunks = adjacent_sentence_clustering(cleaned_documents_2, similarity_threshold=0.5)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Dokument {i+1} hat die folgenden Chunks:\")\n",
    "    for c in chunk:\n",
    "        print(f\"- {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import AgglomerativeClustering, DBSCAN\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "# Lade das deutsche Sprachmodell von spaCy\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "\n",
    "def split_sentences(text):\n",
    "    \"\"\"\n",
    "    Trennt den Text in Sätze unter Verwendung von spaCy.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    return [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Vorverarbeitung des Textes, um unerwünschte Zeichen zu entfernen.\n",
    "    \"\"\"\n",
    "    text = text.replace('\\n', ' ').replace('  ', ' ').strip()\n",
    "    return text\n",
    "\n",
    "def semantic_clustering(documents, method='agglomerative', similarity_threshold=0.5, eps=0.5):\n",
    "    \"\"\"\n",
    "    Führt eine semantische Clusterung von Sätzen in Dokumenten durch.\n",
    "    \n",
    "    Args:\n",
    "    documents (list of str): Liste von Dokumenten, wobei jedes Dokument eine Zeichenkette ist.\n",
    "    method (str): Clustering-Methode ('agglomerative' oder 'dbscan').\n",
    "    similarity_threshold (float): Schwellenwert für die Ähnlichkeit (nur für Agglomerative Clustering).\n",
    "    eps (float): Epsilon-Wert für DBSCAN (nur für DBSCAN).\n",
    "\n",
    "    Returns:\n",
    "    list of list of str: Liste von Clustern (Chunks) für jedes Dokument.\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    clusters = []\n",
    "\n",
    "    for doc in documents:\n",
    "        doc = preprocess_text(doc)\n",
    "        sentences = split_sentences(doc)\n",
    "        \n",
    "        if len(sentences) == 1:\n",
    "            clusters.append([doc])\n",
    "            continue\n",
    "\n",
    "        embeddings = model.encode(sentences)\n",
    "\n",
    "        if method == 'agglomerative':\n",
    "            clustering_model = AgglomerativeClustering(n_clusters=None, distance_threshold=1 - similarity_threshold)\n",
    "        elif method == 'dbscan':\n",
    "            clustering_model = DBSCAN(eps=eps, min_samples=2, metric='cosine')\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported clustering method. Choose 'agglomerative' or 'dbscan'.\")\n",
    "\n",
    "        clustering_model.fit(embeddings)\n",
    "        cluster_labels = clustering_model.labels_\n",
    "\n",
    "        clustered_sentences = {}\n",
    "        for sentence, label in zip(sentences, cluster_labels):\n",
    "            if label == -1:  # DBSCAN Outlier\n",
    "                continue\n",
    "            if label not in clustered_sentences:\n",
    "                clustered_sentences[label] = []\n",
    "            clustered_sentences[label].append(sentence)\n",
    "        \n",
    "        clusters_in_doc = [' '.join(clustered_sentences[label]) for label in np.unique(cluster_labels) if label != -1]\n",
    "        clusters.append(clusters_in_doc)\n",
    "\n",
    "    return clusters\n",
    "\n",
    "\n",
    "# Verwende Agglomerative Clustering\n",
    "chunks_agglomerative = semantic_clustering(cleaned_documents_2, method='agglomerative', similarity_threshold=0.75)\n",
    "for i, chunk in enumerate(chunks_agglomerative):\n",
    "    print(f\"Dokument {i+1} hat die folgenden Chunks (Agglomerative Clustering):\")\n",
    "    for c in chunk:\n",
    "        print(f\"- {c}\")\n",
    "\n",
    "# Verwende DBSCAN\n",
    "chunks_dbscan = semantic_clustering(cleaned_documents_2, method='dbscan', eps=0.3)\n",
    "for i, chunk in enumerate(chunks_dbscan):\n",
    "    print(f\"Dokument {i+1} hat die folgenden Chunks (DBSCAN):\")\n",
    "    for c in chunk:\n",
    "        print(f\"- {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def chunk_documents(documents, chunk_size=8, overlap=2):\n",
    "    \"\"\"\n",
    "    Segmentiert eine Liste von Texten in Sätze und erstellt Chunks mit Overlap.\n",
    "    \n",
    "    :param documents: Liste von Texten (Strings), die segmentiert werden sollen\n",
    "    :param chunk_size: Anzahl der Sätze pro Chunk (z.B. 8 Sätze pro Chunk)\n",
    "    :param overlap: Anzahl der Sätze, die zwischen aufeinanderfolgenden Chunks überlappen sollen (z.B. 2 Sätze)\n",
    "    :return: Liste von Listen, wobei jede innere Liste die Chunks eines Dokuments enthält\n",
    "    \"\"\"\n",
    "    # Lade das deutsche Modell\n",
    "    nlp = spacy.load('de_core_news_sm')\n",
    "    \n",
    "    # Liste zum Speichern der segmentierten und gechunkten Dokumente\n",
    "    chunked_documents = []\n",
    "    \n",
    "    # Verarbeitung und Segmentierung jedes Dokuments in Sätze\n",
    "    for text in documents:\n",
    "        doc = nlp(text)\n",
    "        sentences = list(doc.sents)\n",
    "        \n",
    "        # Chunks erstellen\n",
    "        chunks = []\n",
    "        for i in range(0, len(sentences), chunk_size - overlap):\n",
    "            chunk = sentences[i:i + chunk_size]\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        chunked_documents.append(chunks)\n",
    "    \n",
    "    return chunked_documents\n",
    "\n",
    "def save_chunks_to_file(chunks, filename=\"chunked_output.txt\"):\n",
    "    \"\"\"\n",
    "    Speichert die Chunks in eine Textdatei.\n",
    "    \n",
    "    :param chunks: Liste von gechunkten Dokumenten\n",
    "    :param filename: Der Name der Datei, in die die Chunks geschrieben werden sollen\n",
    "    \"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        for i, doc in enumerate(chunks):\n",
    "            f.write(f\"Dokument {i+1}:\\n\")\n",
    "            for j, chunk in enumerate(doc):\n",
    "                f.write(f\"  Chunk {j+1}:\\n\")\n",
    "                for sentence in chunk:\n",
    "                    f.write(str(sentence) + \"\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "# Aufruf der Funktionen\n",
    "chunked_documents = chunk_documents(cleaned_documents_2, chunk_size=8, overlap=2)\n",
    "save_chunks_to_file(chunked_documents, filename=\"chunked_output.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def convert_strings_to_embeddings(documents_as_strings):\n",
    "    \"\"\"\n",
    "    Konvertiert eine Liste von Strings in Embeddings.\n",
    "    \n",
    "    :param documents_as_strings: Liste von Strings, die in Embeddings umgewandelt werden sollen\n",
    "    :return: Liste von Embeddings\n",
    "    \"\"\"\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "    document_embeddings = embeddings.embed_documents(documents_as_strings)\n",
    "    return document_embeddings\n",
    "\n",
    "def convert_chunks_to_strings(chunked_documents):\n",
    "    \"\"\"\n",
    "    Konvertiert eine Liste von gechunkten Spacy-Spans in eine Liste von Strings.\n",
    "    \n",
    "    :param chunked_documents: Liste von Listen von Spacy-Spans\n",
    "    :return: Liste von Strings, wobei jeder String einen Chunk repräsentiert\n",
    "    \"\"\"\n",
    "    documents_as_strings = []\n",
    "    for doc_chunks in chunked_documents:\n",
    "        for chunk in doc_chunks:\n",
    "            chunk_text = \" \".join([sentence.text for sentence in chunk])\n",
    "            documents_as_strings.append(chunk_text)\n",
    "    return documents_as_strings\n",
    "\n",
    "\n",
    "# Konvertiere die Chunks in Strings\n",
    "documents_as_strings = convert_chunks_to_strings(chunked_documents)\n",
    "print(documents_as_strings)\n",
    "\n",
    "# Konvertiere die Strings in Embeddings\n",
    "#document_embeddings = convert_strings_to_embeddings(documents_as_strings)\n",
    "\n",
    "# Ausgabe der Embeddings für jeden Chunk\n",
    "#for i, embedding in enumerate(document_embeddings):\n",
    "#    print(f\"Embedding für Chunk {i+1}: {embedding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def save_processed_documents(processed_documents, filename):\n",
    "    # Sicherstellen, dass der Ordner \"data/pickle\" existiert\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    \n",
    "    # Speichern der Datei im angegebenen Pfad\n",
    "    with open(filename, \"wb\") as file:\n",
    "        pickle.dump(processed_documents, file)\n",
    "    \n",
    "    print(f\"Die Datei '{filename}' wurde erfolgreich gespeichert.\")\n",
    "\n",
    "# Beispielnutzung:\n",
    "#save_processed_documents(document_embeddings, filename=\"data/pickle/embeddings_for_text.pkl\")\n",
    "save_processed_documents(chunked_documents, filename=\"data/pickle/list_of_text.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_texts_and_embeddings(texts, embeddings):\n",
    "    \"\"\"\n",
    "    Kombiniert eine Liste von Texten mit ihren entsprechenden Embeddings.\n",
    "    \n",
    "    :param texts: Liste von Texten (strings)\n",
    "    :param embeddings: Liste von Embeddings (z.B. Listen von floats)\n",
    "    \n",
    "    :return: Eine Liste von Dictionaries, wobei jedes Dictionary die Struktur hat:\n",
    "             {\n",
    "                 \"id\": <Text Index>,\n",
    "                 \"text\": <Text>,\n",
    "                 \"embedding\": <Embedding>\n",
    "             }\n",
    "    \"\"\"\n",
    "    combined = []\n",
    "    for idx, (text, embedding) in enumerate(zip(texts, embeddings)):\n",
    "        combined.append({\n",
    "            \"id\": f\"id_{idx}\",\n",
    "            \"text\": text,\n",
    "            \"embedding\": embedding\n",
    "        })\n",
    "    \n",
    "    return combined\n",
    "\n",
    "combined_list = combine_texts_and_embeddings(documents_as_strings, document_embeddings)\n",
    "print(combined_list)\n",
    "save_processed_documents(combined_list, filename=\"data/pickle/text_embeddings_combine.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strings_to_documents(documents_as_strings):\n",
    "    # Umwandlung der Strings in Dokumente (in diesem Fall belassen wir die Strings einfach als sie sind)\n",
    "    documents = [doc for doc in documents_as_strings]\n",
    "    \n",
    "    # Ausgabe der Dokumente\n",
    "    for i, doc in enumerate(documents):\n",
    "        print(f\"Dokument {i+1}:\")\n",
    "        print(doc)\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "        \n",
    "import numpy as np\n",
    "\n",
    "def convert_and_print_embeddings(document_embeddings):\n",
    "    \"\"\"\n",
    "    Konvertiert eine Liste von Dokumenten-Einbettungen in ein NumPy-Array und gibt die Einbettungen aus.\n",
    "    \n",
    "    Args:\n",
    "    document_embeddings (list): Eine Liste von Dokumenten-Einbettungen.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Das konvertierte NumPy-Array.\n",
    "    \"\"\"\n",
    "    # Konvertiere die Liste in ein NumPy-Array\n",
    "    embeddings_np = np.array(document_embeddings)\n",
    "    \n",
    "    # Ausgabe der Einbettungen\n",
    "    for i, embedding in enumerate(embeddings_np):\n",
    "        print(f\"Dokument {i+1} Einbettung:\")\n",
    "        print(embedding)\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "    \n",
    "    return embeddings_np\n",
    "docs_finished = documents_as_strings\n",
    "emb_finished = convert_and_print_embeddings(document_embeddings)\n",
    "\n",
    "save_processed_documents(emb_finished, filename=\"data/pickle/emb_finished.pkl\")\n",
    "save_processed_documents(documents_as_strings, filename=\"data/pickle/docs_finished.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings\n",
    "\n",
    "def delete_chroma_collection(collection_name='document_embeddings_v7', persist_directory='chroma_db'):\n",
    "    # Öffne die Chroma-Datenbank mit Persistierung\n",
    "    client = chromadb.PersistentClient(\n",
    "        path=persist_directory,\n",
    "        settings=Settings(),\n",
    "        tenant=DEFAULT_TENANT,\n",
    "        database=DEFAULT_DATABASE,\n",
    "    )\n",
    "    \n",
    "    # Überprüfe, ob die Collection existiert, und lösche sie, wenn ja\n",
    "    existing_collections = [col.name for col in client.list_collections()]\n",
    "    if collection_name in existing_collections:\n",
    "        client.delete_collection(name=collection_name)\n",
    "        print(f\"Collection '{collection_name}' erfolgreich gelöscht.\")\n",
    "    else:\n",
    "        print(f\"Collection '{collection_name}' nicht gefunden.\")\n",
    "        \n",
    "delete_chroma_collection(collection_name='document_embeddings_v7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2\n",
    "import chromadb\n",
    "from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings\n",
    "\n",
    "def create_chroma_database_v2(embeddings, documents, collection_name='document_embeddings_v7', persist_directory='chroma_db'):\n",
    "    # Erstelle oder öffne eine Chroma-Datenbank mit Persistierung\n",
    "    client = chromadb.PersistentClient(\n",
    "        path=persist_directory,\n",
    "        settings=Settings(),\n",
    "        tenant=DEFAULT_TENANT,\n",
    "        database=DEFAULT_DATABASE,\n",
    "    )\n",
    "    \n",
    "    # Erstelle eine neue Collection in der Datenbank oder lade eine bestehende\n",
    "    collection = client.get_or_create_collection(name=collection_name)\n",
    "    \n",
    "    # Konvertiere Embeddings von numpy array in Liste von Listen\n",
    "    embeddings_as_list = [embedding.tolist() for embedding in embeddings]\n",
    "    \n",
    "    # Dokumente und Embeddings zur Collection hinzufügen\n",
    "    for i, (embedding, document) in enumerate(zip(embeddings_as_list, documents)):\n",
    "        collection.add(\n",
    "            embeddings=[embedding],\n",
    "            documents=[document],\n",
    "            ids=[f\"doc_{i}\"]\n",
    "        )\n",
    "    \n",
    "    print(f\"Chroma-Datenbank '{collection_name}' erfolgreich erstellt und {len(embeddings)} Embeddings und Dokumente hinzugefügt.\")\n",
    "\n",
    "# Beispielnutzung:\n",
    "create_chroma_database_v2(emb_finished, docs_finished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1\n",
    "import chromadb\n",
    "from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings\n",
    "\n",
    "def create_chroma_database_v2(combined_list, collection_name='document_embeddings_v7', persist_directory='chroma_db'):\n",
    "    # Erstelle oder öffne eine Chroma-Datenbank mit Persistierung\n",
    "    client = chromadb.PersistentClient(\n",
    "        path=persist_directory,\n",
    "        settings=Settings(),\n",
    "        tenant=DEFAULT_TENANT,\n",
    "        database=DEFAULT_DATABASE,\n",
    "    )\n",
    "    \n",
    "    # Erstelle eine neue Collection in der Datenbank oder lade eine bestehende\n",
    "    collection = client.get_or_create_collection(name=collection_name)\n",
    "    \n",
    "    # Füge die Embeddings und Texte (als Dokumente) zur Collection hinzu\n",
    "    for entry in combined_list:\n",
    "        embedding_as_list = [entry['embedding']]  # Einzeln in eine Liste einbetten\n",
    "        collection.add(\n",
    "            embeddings=embedding_as_list,\n",
    "            documents=[entry['text']],  # Text als Dokument hinzugefügt\n",
    "            ids=[entry['id']],\n",
    "        )\n",
    "    \n",
    "    print(f\"Chroma-Datenbank '{collection_name}' erfolgreich erstellt und {len(combined_list)} Embeddings hinzugefügt.\")\n",
    "\n",
    "# Beispielnutzung:\n",
    "create_chroma_database_v2(combined_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "def query_first_element(collection_name='document_embeddings_v7', persist_directory='chroma_db', document_id='id_0'):\n",
    "    # Erstelle oder öffne eine Chroma-Datenbank mit Persistierung\n",
    "    client = chromadb.PersistentClient(\n",
    "        path=persist_directory,\n",
    "        settings=chromadb.config.Settings(),\n",
    "    )\n",
    "    \n",
    "    # Lade die Collection aus der Datenbank\n",
    "    collection = client.get_collection(name=collection_name)\n",
    "    \n",
    "    # Führe die Abfrage durch, um das Dokument und das Embedding mit der spezifischen ID abzurufen\n",
    "    result = collection.get(ids=[document_id])\n",
    "    \n",
    "    # Das Ergebnis anzeigen\n",
    "    if result and 'documents' in result and result['documents']:\n",
    "        print(f\"Document ID: {document_id}\")\n",
    "        print(f\"Text: {result['documents'][0]}\")\n",
    "        if 'embeddings' in result and result['embeddings']:\n",
    "            print(f\"Embedding: {result['embeddings'][0]}\")\n",
    "        else:\n",
    "            print(\"No embedding found for this document.\")\n",
    "    else:\n",
    "        print(f\"No document found with ID: {document_id}\")\n",
    "\n",
    "# Beispielnutzung:\n",
    "query_first_element()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def query_similar_documents(query_text, collection_name='document_embeddings_v7', persist_directory='chroma_db', model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'):\n",
    "    # Lade das Embedding-Modell\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # Erstelle oder öffne eine Chroma-Datenbank mit Persistierung\n",
    "    client = chromadb.PersistentClient(\n",
    "        path=persist_directory,\n",
    "        settings=Settings(),\n",
    "    )\n",
    "    \n",
    "    # Lade die Collection aus der Datenbank\n",
    "    collection = client.get_collection(name=collection_name)\n",
    "    \n",
    "    # Erstelle ein Embedding für den Abfrage-Text\n",
    "    query_embedding = model.encode(query_text).tolist()\n",
    "    \n",
    "    # Führe die Abfrage durch, um ähnliche Dokumente zu finden\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=3,  # Anzahl der zurückgegebenen Ergebnisse, du kannst diese Zahl anpassen\n",
    "        include=[\"distances\"]  # Fügt den Ähnlichkeits-Score in die Ergebnisse ein\n",
    "    )\n",
    "    \n",
    "    # Debugging: Ausgabe der Struktur von 'results'\n",
    "    print(\"Abfrageergebnisse:\", results)\n",
    "    \n",
    "    # Das Ergebnis anzeigen\n",
    "    if results and 'documents' in results and results['documents']:\n",
    "        for i, document in enumerate(results['documents'][0]):\n",
    "            print(f\"Ergebnis {i+1}:\")\n",
    "            print(f\"Text: {document}\")\n",
    "            print(f\"ID: {results['ids'][0][i]}\")\n",
    "            print(f\"Score (Distanz): {results['distances'][0][i]}\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"Keine ähnlichen Dokumente gefunden.\")\n",
    "\n",
    "# Beispielnutzung:\n",
    "query_similar_documents(\"was kommt in den Altglascontainer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predictiv_chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
