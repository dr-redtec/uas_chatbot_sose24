{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Lade das deutsche Sprachmodell von spaCy\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "def extract_keywords_from_query(query, custom_stopwords=None):\n",
    "    \"\"\"\n",
    "    Extrahiert Keywords aus einer Query durch Tokenisierung und Entfernung von Stoppwörtern.\n",
    "    \"\"\"\n",
    "    if custom_stopwords is None:\n",
    "        custom_stopwords = set(nlp.Defaults.stop_words)\n",
    "    \n",
    "    # Tokenisiere die Query\n",
    "    doc = nlp(query.lower())\n",
    "    \n",
    "    # Extrahiere Keywords: Wörter, die keine Stoppwörter, Zahlen oder Satzzeichen sind\n",
    "    keywords = [token.text for token in doc if token.text not in custom_stopwords and not token.is_punct and not token.like_num]\n",
    "    \n",
    "    return keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jan/Documents/GitHub/predictiv_chatbot/predictiv_chatbot/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/Users/jan/Documents/GitHub/predictiv_chatbot/predictiv_chatbot/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Number of requested results 10 is greater than number of elements in index 8, updating n_results = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection document_embeddings_v2 erfolgreich geladen.\n",
      "Query: Was kommt in die gelbe Tonne?\n",
      "Extrahierte Keywords: ['gelbe', 'tonne']\n",
      "Ergebnis 1:\n",
      "\n",
      "ID: doc_7\n",
      "Text: gelbe tonne gelben sack gehören gebrauchte restentleerte verpackungen nicht papier glas verpackungsbestandteile trennen ausspülen nicht menüschalen kunststoffdeckel fertiggerichten arzneimittelblister müsliriegelfolie joghurtbecher nudeltüten einkaufstüten reinigungsmittelflaschen gemüsebeutel kunststoff flüssigseife eisverpackungen fruchtpüree üllmaterial versand verpackungen shampooflaschen luftpolsterfolie spraydosen soßentüten konservendosen tierfutterdosen kronkorken zahnpastatuben kunststoffschalen lebensmittel nicht gelbe tonne gelben sack gehören verpackungen papier pappe glas abfälle verpackungen schuhe altkleider gummi strumpfhosen batterien holzwolle styroporreste behälterglas dämmplatten blechgeschirr tapetenreste disketten keramikprodukte windeln druckerpatronen kinderspielzeug zahnbürsten klarsichthüllen zigarettenkippen elektrogeräte kugelschreiber essensreste nicht restentleerte verpackungen faltschachteln papier pappe papiertaschentücher ilme dvds pflaster videokassetten porzellan energiesparlampen bioabfälle regionale ausnahmen wertstofftonne nähere infos klicken zuständige ansprechpartner\n",
      "Keywords: ['gelbe', 'tonne']\n",
      "Score: 1.0\n",
      "\n",
      "==================================================\n",
      "\n",
      "Ergebnis 2:\n",
      "\n",
      "ID: doc_6\n",
      "Text: wertstofftonne gehören gebrauchte restlentleerte verpackungen nicht papier glas gegenstände kunststoff metall trennen ausspülen nicht deckel saftflaschen schaumstoffe arzneimittelblister schrauben esteck flaschenverschlüsse töpfe folien werkzeuge ebrauchsgegenstände gießkannen blumentöpfe plastikschüsseln getränkedosen getränkekartons margarinebecher konservendosen unststoffflaschen pflegemittel waschmittelflaschen icht wertstofftonne gehören verpackungen papier pappe glas restabfälle verpackte alte keramik abgelagerte lebensmittel batterien windeln bauabfälle zigarettenkippen medikamentenreste datenträger nasenschutz elektrogeräte porzellan energiesparlampen schuhe gummi spritzen holz holzreste strumpfhosen hygieneartikel windeln styroporreste dämmplatten binden wattereste tapetenreste kondome textilien audiokassetten kanülen regionale ausnahmen nähere infos klicken zuständige ansprechpartner\n",
      "Keywords: ['gelbe', 'tonne']\n",
      "Score: 0.6789525617641201\n",
      "\n",
      "==================================================\n",
      "\n",
      "Ergebnis 3:\n",
      "\n",
      "ID: doc_3\n",
      "Text: gehört bioabfall gehört biotonne gemüsereste kaffeesatz filter teebeutel essensreste gekochte alte lebensmittel verpackung eierschalen blumen gartenabfälle laub einwickelpapier küchenpapier nicht biotonne kompostierbare plastiktüten erde holz grassoden sand steine scannen erhalten informationen sprachen gehört restmülltonne zigarettenkippen asche kehricht keramik porzellan tonteller töpfe putzlappen schwämme nicht restmülltonne schadstoffe batterien elektrogeräte bauschutt entsorgungsbetriebe gehört zeitungen zeitschriften kataloge prospekte hefte bücher packpapier unbeschichtet pappe nicht papiertonne holz schmutziges fotos plastiktüten fran gelber sack papiertonne gehört margarinebecher verpackungsstyropor kunststofftuben kunststoffflaschen plastiktüten konservendosen safttüten vakuumverpackungen nicht gelben plastikschüsseln beschichtetes papier kleiderbügel kunststoffrohre kurd gelben sack gehört glascontainer weißglas buntglas einwegglasflaschen sonstiges hohlglas nutzen wertstoffhöfe fragen mülltrennung beraten sack nicht fensterscheiben porzellan keramik glühbirnen entsorgungluebeck entsorgungsbetriebe ebhl\n",
      "Keywords: ['gelbe', 'tonne']\n",
      "Score: 0.6432961416024562\n",
      "\n",
      "==================================================\n",
      "\n",
      "Ergebnis 4:\n",
      "\n",
      "ID: doc_1\n",
      "Text: wohnsiedlungen sperrmüll abgeholt termine nden schwarzen wertstoffe entsorgen ttrü nden abfalltonnen haus dauer nicht ausreichen hausbesitzer größere bestellen internet spezialbehälter besondere abfälle baustellenabfälle erfahren servicetelefonnummer kosten sperrmüll abfallgebühr erhalten wochen haus näheres gelbe verpackungstonne verpackungen symbol grünen nicht papier glas bestehen nicht gespült verpackungen kunststoff folien becher verbundverpackungen papier milchtüten verpackungen metall getränkedosen geschäumte kunststoffe gemüseverpackungen nicht verpackungen papier pappe altpapiertonne verpackungen glas altglascontainer verschmutzte verpackungen gelben tonne nicht aussortiert weiterverarbeitet standort leerung grüne altpapiertonne punkt grüne altpapiertonne gehören papier pappe beschichtung unverschmutzt pappen kartonagen styropor zeitungen illustrierte kataloge bücher hefte nicht verschmutztes papier restmüll beschichtetes papier restmüll hygienepapiere windeln restmüll näheres sichere aktenvernichtung erfahren servicetelefonnummer wertstoffe standort leerung braune biotonne abbaubaren abfälle privaten haushalten feuchte bioabfälle gemüsereste küchenpapier wickeln schützen tonne starker verschmutzung gemüseabfälle zitrusfrüchte nussschalen rohe gekochte lebensmittelreste teesatz filtertüten eierschalen grünschnitt laub nicht plastiktüten verpackungen kunststoff metall gelbe verpackungstonne windeln gebrauchte hygieneartikel restmüll standort leerung graue restmülltonne\n",
      "Keywords: ['gelbe', 'tonne']\n",
      "Score: 0.42968080107154305\n",
      "\n",
      "==================================================\n",
      "\n",
      "Ergebnis 5:\n",
      "\n",
      "ID: doc_0\n",
      "Text: blaue altglascontainer saubere flaschen deckelgläser farben weiß getrennt blaues buntes glas bitte grünglas geben aschen konservengläser nicht scheiben trinkgläser blumenvasen restmüll keramik glühbirnen restmüll energiesparlampen wertstoffhöfe verschmutzte gläser restmüll standort öffentlichen sperrmüll plätzen frankfurter trennen service gmbh ecke töngesgasse schärfengässchen rgungsfachbetrie rtschafte eddn trennen haus gehören abfallbehälter tonnen inhalt produkten weiterverarbeitet graue restmülltonne gelbe tonne verpackungen grüne tonne papier pappe braune tonne bioabfälle graue tonne restlichen abfall folgende entsorgungsangebote wertstoffe sichern altglascontainer flaschen deckelgläser nden öffentlichen plätzen altkleider container altkleider schuhe nden stadtteil öffentlichen plätzen schadstoffe abfälle gefährlichen stoffen farben spraydosen mitteln abgeben termine nden haushalte verteilt sperrmüllabholung holt termin wohnsiedlungen sperrmüll abgeholt termine nden schwarzen wertstoffe entsorgen ttrü nden abfalltonnen haus dauer nicht ausreichen hausbesitzer größere bestellen internet spezialbehälter besondere abfälle baustellenabfälle erfahren servicetelefonnummer kosten sperrmüll abfallgebühr erhalten wochen haus näheres gelbe verpackungstonne verpackungen symbol grünen nicht papier glas bestehen nicht gespült verpackungen kunststoff folien becher verbundverpackungen papier milchtüten\n",
      "Keywords: ['gelbe', 'tonne']\n",
      "Score: 0.41812791160928175\n",
      "\n",
      "==================================================\n",
      "\n",
      "Ergebnis 6:\n",
      "\n",
      "ID: doc_5\n",
      "Text: papiertonne gehören verpackungen papier anhaftung speiseresten verpackungen zerkleinert zusammengelegt gefaltet verpackungen briefe brötchen metzger briefumschläge obsttüten bücher eierschachteln geschenkpapier faltschachteln form kataloge postkarten papier schulhefte zuckertüten werbeprospekte nudelkartons zeitschriften papiertragetaschen zeitungen pappummantelung joghurtbechern pralinenschachteln nicht gehören abfälle verpackungen nicht papier batterien luftpolster bioabfall dosen soßentüten fotos pezialpapiere backpapier glas thermopapier holzschachteln holzwolle tapeten kontoauszüge erschmutzte volle thermopapiere verpackungen küchenabfälle sektkorken papiertaschentücher kunststoffe regionale ausnahmen nähere infos klicken zuständige ansprechpartner\n",
      "Keywords: ['gelbe', 'tonne']\n",
      "Score: 0.30606574682254417\n",
      "\n",
      "==================================================\n",
      "\n",
      "Ergebnis 7:\n",
      "\n",
      "ID: doc_4\n",
      "Text: gehören restentleerte glasverpackungen farben weiß sortieren vorgesehenen container werfen nicht zuordenbare farben blaues glas deckel nicht abgeschraubt nicht bepfandeten glasflaschen wein fruchtnektare glas parfümflaschen marmeladen senfgläser sonstiges verpackungsglas obst soßen suppen gemüse glas nicht gehören abfälle glas verpackungen auflaufformen hitzebeständiges glas steingutflaschen autolampen isolierglas teller tassen autoscheiben kaffeekannen trinkgläser batterien ofenglas restabfall keramik bleiglas blumentöpfe mikrowellengeschirr flachglas produkte verpackungsbestandteile glaskeramik kunststoff glaskochplatten glühbirnen spritzen regionale ausnahmen nähere infos klicken zuständige ansprechpartner\n",
      "Keywords: ['gelbe', 'tonne']\n",
      "Score: 0.03132202624598668\n",
      "\n",
      "==================================================\n",
      "\n",
      "Ergebnis 8:\n",
      "\n",
      "ID: doc_2\n",
      "Text: restmüll näheres sichere aktenvernichtung erfahren servicetelefonnummer wertstoffe standort leerung braune biotonne abbaubaren abfälle privaten haushalten feuchte bioabfälle gemüsereste küchenpapier wickeln schützen tonne starker verschmutzung gemüseabfälle zitrusfrüchte nussschalen rohe gekochte lebensmittelreste teesatz filtertüten eierschalen grünschnitt laub nicht plastiktüten verpackungen kunststoff metall gelbe verpackungstonne windeln gebrauchte hygieneartikel restmüll standort leerung graue restmülltonne abfälle verunreinigung vermischung tonnen entsorgt schmutzte lebensmittelverpackungen damenbinden babywindeln staub asche kehricht zigarettenkippen küchentücher putzlappen trinkgläser porzellan keramik scherben nicht elektrogeräte schrott wertstoffhöfe großgeräte sperrmüll batterien spezielle sammelbehälter supermärkten öffentlichen gebäuden sonderabfall schadstoffe farben lacke standort leerung spiegel\n",
      "Keywords: ['gelbe', 'tonne']\n",
      "Score: 0.0\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings\n",
    "\n",
    "# Lade das vortrainierte Modell\n",
    "model_name = 'sentence-transformers/multi-qa-mpnet-base-dot-v1'\n",
    "embedding_model = SentenceTransformer(model_name)\n",
    "\n",
    "# Initialisiere den Chroma-Client\n",
    "chroma_client = chromadb.PersistentClient(\n",
    "    path=\"chroma_db\",\n",
    "    settings=Settings(),\n",
    "    tenant=DEFAULT_TENANT,\n",
    "    database=DEFAULT_DATABASE,\n",
    ")\n",
    "\n",
    "\n",
    "# Überprüfe, ob die Collection existiert\n",
    "def get_existing_collection(chroma_client, collection_name=\"document_embeddings_v2\"):\n",
    "    try:\n",
    "        collection = chroma_client.get_collection(name=collection_name)\n",
    "        print(f\"Collection {collection_name} erfolgreich geladen.\")\n",
    "        return collection\n",
    "    except ValueError:\n",
    "        print(f\"Collection {collection_name} existiert nicht.\")\n",
    "        return None\n",
    "\n",
    "# Zugriff auf die bestehende Collection\n",
    "collection = get_existing_collection(chroma_client, \"document_embeddings_v2\")\n",
    "\n",
    "if collection is None:\n",
    "    print(\"Die angegebene Collection existiert nicht. Bitte überprüfe den Collection-Namen.\")\n",
    "else:\n",
    "    # Erweiterte Version der hybriden Suche mit normalisierter Distanzberechnung, Version 5\n",
    "    def hybrid_search_v5(chroma_client, query, keywords, embedding_model, collection_name=\"document_embeddings_v2\", top_k=10):\n",
    "        # 1. Semantische Suche\n",
    "        query_embedding = embedding_model.encode(query).tolist()\n",
    "        collection = chroma_client.get_collection(name=collection_name)\n",
    "        semantic_results = collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=top_k\n",
    "        )\n",
    "        \n",
    "        # 2. Distanznormalisierung\n",
    "        distances = semantic_results['distances'][0]\n",
    "        min_distance = min(distances)\n",
    "        max_distance = max(distances)\n",
    "        normalized_distances = [(d - min_distance) / (max_distance - min_distance) for d in distances]\n",
    "        \n",
    "        # 3. Klassische Textsuche basierend auf Keywords\n",
    "        text_search_results = []\n",
    "        for i in range(len(semantic_results['ids'][0])):\n",
    "            metadata = semantic_results['metadatas'][0][i]\n",
    "            keyword_count = sum(keyword.lower() in metadata['text'].lower() for keyword in keywords)\n",
    "            if keyword_count > 0:\n",
    "                text_search_results.append({\n",
    "                    'id': semantic_results['ids'][0][i],\n",
    "                    'text': metadata['text'],\n",
    "                    'keywords': keywords,\n",
    "                    'score': 1.0 + 0.5 * keyword_count  # Gewichtung für Keywords\n",
    "                })\n",
    "        \n",
    "        # 4. Kombination der Ergebnisse\n",
    "        combined_results = []\n",
    "        seen_ids = set()\n",
    "        for i in range(len(semantic_results['ids'][0])):\n",
    "            distance = normalized_distances[i]\n",
    "            combined_score = 1.0 - distance  # Gewichtung für semantische Suche\n",
    "            \n",
    "            if semantic_results['ids'][0][i] not in seen_ids:\n",
    "                seen_ids.add(semantic_results['ids'][0][i])\n",
    "                combined_results.append({\n",
    "                    'id': semantic_results['ids'][0][i],\n",
    "                    'text': semantic_results['metadatas'][0][i]['text'] if isinstance(semantic_results['metadatas'][0][i], dict) else \"\",\n",
    "                    'keywords': keywords,\n",
    "                    'score': combined_score\n",
    "                })\n",
    "        \n",
    "        for result in text_search_results:\n",
    "            if result['id'] not in seen_ids:\n",
    "                combined_results.append(result)\n",
    "        \n",
    "        # 5. Sortieren der kombinierten Ergebnisse nach Score\n",
    "        combined_results.sort(key=lambda x: x['score'], reverse=True)\n",
    "        \n",
    "        # 6. Ausgabe der Top-k Ergebnisse\n",
    "        return combined_results[:top_k]\n",
    "\n",
    "    # Beispielnutzung:\n",
    "    query5 = \"Was kommt in die gelbe Tonne?\"\n",
    "    # query5 = \"Wie entsorge ich Altglas in Frankfurt?\"\n",
    "\n",
    "    keywords5 = extract_keywords_from_query(query5)\n",
    "\n",
    "    # Durchführung der hybriden Suche mit Keywords\n",
    "    hybrid_results_v5 = hybrid_search_v5(chroma_client, query5, keywords5, embedding_model)\n",
    "\n",
    "    print(f\"Query: {query5}\")\n",
    "    print(f\"Extrahierte Keywords: {keywords5}\")\n",
    "    # Ausgabe der hybriden Suchergebnisse\n",
    "    for i, result in enumerate(hybrid_results_v5):\n",
    "        print(f\"Ergebnis {i+1}:\\n\")\n",
    "        print(f\"ID: {result['id']}\")\n",
    "        print(f\"Text: {result['text']}\")\n",
    "        print(f\"Keywords: {result.get('keywords', [])}\")\n",
    "        print(f\"Score: {result['score']}\")\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispielnutzung:\n",
    "query5 = \"Wie entsorge ich Altglas in Frankfurt?\"\n",
    "keywords5 = extract_keywords_from_query(query5)\n",
    "\n",
    "# Durchführung der hybriden Suche mit Keywords\n",
    "hybrid_results_v5 = hybrid_search_v5(chroma_client, query5, keywords5, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "\n",
    "# Verwende das deutschsprachige Modell\n",
    "model_name = \"distilbert-base-german-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Initialisiere den Ollama-Client mit den angegebenen Daten\n",
    "ollama_llm = Ollama(\n",
    "    model=\"llama3.1:8b\",\n",
    "    base_url=\"http://192.168.180.131:11434\"\n",
    ")\n",
    "\n",
    "# Initialisiere die Pipeline für Textklassifikation auf der GPU (falls verfügbar)\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "def extract_relevant_information_transformers(text, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Verwendet ein Hugging Face Transformer-Modell, um relevante Informationen aus dem Text zu extrahieren.\n",
    "    \"\"\"\n",
    "    sentences = text.split('. ')\n",
    "    relevant_info = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Verwende das Modell, um die Relevanz jedes Satzes zu bestimmen\n",
    "        result = classifier(sentence)\n",
    "        # Füge relevante Sätze hinzu, basierend auf einem Schwellenwert\n",
    "        if result[0]['score'] > threshold:\n",
    "            relevant_info.append(sentence)\n",
    "    \n",
    "    return \" \".join(relevant_info)\n",
    "\n",
    "def build_filtered_context_transformers(hybrid_results, top_n=3):\n",
    "    \"\"\"\n",
    "    Filtert und baut einen Kontextstring aus den besten Suchergebnissen auf,\n",
    "    wobei ein Transformer-Modell zur Extraktion relevanter Informationen verwendet wird.\n",
    "    \"\"\"\n",
    "    top_results = hybrid_results[:top_n]\n",
    "    \n",
    "    # Verwende das Transformer-Modell, um relevante Informationen zu extrahieren\n",
    "    filtered_context = \"\\n\\n\".join([\n",
    "        f\"Result {i+1}:\\n{extract_relevant_information_transformers(result['text'])}\" \n",
    "        for i, result in enumerate(top_results)\n",
    "    ])\n",
    "    \n",
    "    return filtered_context\n",
    "\n",
    "def ask_llm_with_ollama(query, filtered_context, llm):\n",
    "    \"\"\"\n",
    "    Sendet eine Query an das LLM (Ollama), zusammen mit dem gefilterten Kontext.\n",
    "    \n",
    "    Parameters:\n",
    "    - query: Die ursprüngliche Frage.\n",
    "    - filtered_context: Der gefilterte Kontext, der für die Beantwortung verwendet werden soll.\n",
    "    - llm: Das LLM-Objekt, das die Anfrage bearbeitet.\n",
    "    \n",
    "    Returns:\n",
    "    - Die Antwort des LLM.\n",
    "    \"\"\"\n",
    "    prompt_text = f\"\"\"\n",
    "        Frage: '{query}'\n",
    "\n",
    "        Der folgende Text enthält wesentliche Informationen zur Mülltrennung in Deutschland. Beachte die spezifischen Kategorien und entscheide, welche Materialien in welche Tonne gehören.\n",
    "\n",
    "        Kategorien:\n",
    "        - Gelbe Tonne: Kunststoffverpackungen, Metallverpackungen, Verbundstoffe.\n",
    "        - Papiertonne: Papier, Pappe.\n",
    "        - Glascontainer: Glasflaschen, Gläser.\n",
    "        - Restmüll: Dinge, die nicht recycelt werden können.\n",
    "        - Biotonne: Organische Abfälle wie Lebensmittelreste und Gartenabfälle.\n",
    "\n",
    "        Kontext:\n",
    "\n",
    "        {filtered_context}\n",
    "\n",
    "        Antwort:\n",
    "        \"\"\"\n",
    "    \n",
    "    response = llm(prompt_text)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Beispielnutzung:\n",
    "query5 = \"Was kommt in die gelbe Tonne?\"\n",
    "keywords5 = extract_keywords_from_query(query5)\n",
    "\n",
    "# Durchführung der hybriden Suche mit Keywords\n",
    "hybrid_results_v5 = hybrid_search_v5(chroma_client, query5, keywords5, embedding_model)\n",
    "\n",
    "# Baue den gefilterten Kontext mit Transformers\n",
    "filtered_context_transformers = build_filtered_context_transformers(hybrid_results_v5, top_n=3)\n",
    "\n",
    "# Sende die Query und den Transformer-gefilterten Kontext an Ollama\n",
    "llm_response = ask_llm_with_ollama(query5, filtered_context_transformers, ollama_llm)\n",
    "\n",
    "# Ausgabe der Antwort des LLM\n",
    "print(\"Antwort des LLM:\")\n",
    "print(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jan/Documents/GitHub/predictiv_chatbot/predictiv_chatbot/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-german-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Number of requested results 10 is greater than number of elements in index 8, updating n_results = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wie entsorge ich Batterien?\n",
      "Antwort des Llama 3.1 8B:\n",
      "Um Batterien ordnungsgemäß zu entsorgen, sollten sie aus Elektrogeräten entfernt und in spezielle Sammelboxen oder -stellen gegeben werden. Es besteht auch die Möglichkeit, sie in Wertstoffhöfen oder bei freigestellten Einrichtungen wie Tankstellen, Supermärkten und Drogeriemärkten abzugeben. Bei Fragen zur korrekten Mülltrennung kann an die kommunale Abfallberatung herangetreten werden. Altpapier hingegen sollte in der entsprechenden Container mit der blauen Tonne entsorgt werden, zusammen mit leeren Verpackungen, Zeitungen, Zeitschriften und Schulheften.\n",
      "Ergebnisse aus RAG\n",
      "Result 1:\n",
      "batterien akkumulatoren gehören nicht brände auslösen umweltgefährdende stoffe enthalten mensch umwelt belasten ressourcen rohstoffe sammelboxen handel batterien verkauft supermärkte drogeriemärkte warenhäuser baumärkte tankstellen einpacktische kommunale sammelstellen wertstoffhöfe schadstoffmobile freiwillige sammelstellen unternehmen behörden hochschulen nicht batterien elektrogeräten entsorgung entnehmen batteriesammlung geben\n",
      "\n",
      "Result 2:\n",
      "trennen abfall abfalls korrekte mülltrennung verpackungen elektrogeräte batterien bioabfall entsorgt regionale besonderheiten wertstofftonne nähere informationen erteilt kommunale abfallberatung\n",
      "\n",
      "Result 3:\n",
      "altpapier container blaue tonne papier leere verpackungen zeitschriften zeitungen schulhefte\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "\n",
    "# Verwende das deutschsprachige Modell\n",
    "model_name = \"distilbert-base-german-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Initialisiere den Ollama-Client mit den angegebenen Daten\n",
    "ollama_llm = Ollama(\n",
    "    model=\"llama3.1:8b\",\n",
    "    base_url=\"http://192.168.180.131:11434\"\n",
    ")\n",
    "\n",
    "# Initialisiere die Pipeline für Textklassifikation auf der GPU (falls verfügbar)\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "def extract_relevant_information_transformers(text, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Verwendet ein Hugging Face Transformer-Modell, um relevante Informationen aus dem Text zu extrahieren.\n",
    "    \"\"\"\n",
    "    sentences = text.split('. ')\n",
    "    relevant_info = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Verwende das Modell, um die Relevanz jedes Satzes zu bestimmen\n",
    "        result = classifier(sentence)\n",
    "        # Füge relevante Sätze hinzu, basierend auf einem Schwellenwert\n",
    "        if result[0]['score'] > threshold:\n",
    "            relevant_info.append(sentence)\n",
    "    \n",
    "    return \" \".join(relevant_info)\n",
    "\n",
    "def build_filtered_context_transformers(hybrid_results, top_n=3):\n",
    "    \"\"\"\n",
    "    Filtert und baut einen Kontextstring aus den besten Suchergebnissen auf,\n",
    "    wobei ein Transformer-Modell zur Extraktion relevanter Informationen verwendet wird.\n",
    "    \"\"\"\n",
    "    top_results = hybrid_results[:top_n]\n",
    "    \n",
    "    # Verwende das Transformer-Modell, um relevante Informationen zu extrahieren\n",
    "    filtered_context = \"\\n\\n\".join([\n",
    "        f\"Result {i+1}:\\n{extract_relevant_information_transformers(result['text'])}\" \n",
    "        for i, result in enumerate(top_results)\n",
    "    ])\n",
    "    \n",
    "    return filtered_context\n",
    "\n",
    "def ask_llm_with_ollama(query, filtered_context, llm):\n",
    "    \"\"\"\n",
    "    Sendet eine Query an das LLM (Ollama), zusammen mit dem gefilterten Kontext.\n",
    "    \n",
    "    Parameters:\n",
    "    - query: Die ursprüngliche Frage.\n",
    "    - filtered_context: Der gefilterte Kontext, der für die Beantwortung verwendet werden soll.\n",
    "    - llm: Das LLM-Objekt, das die Anfrage bearbeitet.\n",
    "    \n",
    "    Returns:\n",
    "    - Die Antwort des LLM.\n",
    "    \"\"\"\n",
    "    prompt_text = f\"\"\"\n",
    "    Frage: \n",
    "\n",
    "\n",
    "    Antwort:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm(prompt_text)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# '{query}'\n",
    "\n",
    "#    Bitte beantworte die folgende Frage direkt und spezifisch, basierend auf den bereitgestellten Informationen zur Mülltrennung. Stelle sicher, dass deine Antwort präzise, klar und ausschließlich auf den untenstehenden Kontext gestützt ist. Achte besonders auf Wörter wie 'außer', 'nicht' oder ähnliche Ausdrücke, die auf Ausnahmen oder Ausschlüsse hinweisen, um sicherzustellen, dass diese korrekt in die Antwort integriert werden. Vermeide Spekulationen oder Informationen, die nicht im Kontext enthalten sind. Gib die Antwort als Fließtext, ohne zu erwähnen, dass du aus einem Kontext zitierst oder auf spezifische Quellen verweist.\n",
    "\n",
    "#    Kontext:\n",
    "\n",
    "#    {filtered_context}\n",
    "# Beispielnutzung:\n",
    "#query5 = \"Was kommt in die gelbe Tonne? Und was kommt in den Restmüll?\"\n",
    "query5 = \"Wie entsorge ich Batterien?\"\n",
    "\n",
    "#query5 = \"Wie werden Metalle wie Dosen und Kronkorken entsorgt?\"\n",
    "keywords5 = extract_keywords_from_query(query5)\n",
    "\n",
    "# Durchführung der hybriden Suche mit Keywords\n",
    "hybrid_results_v5 = hybrid_search_v5(chroma_client, query5, keywords5, embedding_model)\n",
    "\n",
    "# Baue den gefilterten Kontext mit Transformers\n",
    "filtered_context_transformers = build_filtered_context_transformers(hybrid_results_v5, top_n=3)\n",
    "\n",
    "# Sende die Query und den Transformer-gefilterten Kontext an Ollama\n",
    "llm_response = ask_llm_with_ollama(query5, filtered_context_transformers, ollama_llm)\n",
    "\n",
    "# Ausgabe der Antwort des LLM\n",
    "print(query5)\n",
    "print(\"Antwort des Llama 3.1 8B:\")\n",
    "print(llm_response)\n",
    "print(\"Ergebnisse aus RAG\")\n",
    "print(filtered_context_transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batterien sollten in spezielle Sammelboxen oder sammelstellen, wie z.B. Tankstellen, Drogeriemärkte oder Baumärkte, gegeben werden und nicht mit Elektrogeräten entsorgt werden. Wenn du mehr über die korrekte Mülltrennung erfahren möchtest, kannst du dich an die kommunale Abfallberatung wenden.\n"
     ]
    }
   ],
   "source": [
    "llm_response = ask_llm_with_ollama(query5, filtered_context_transformers, ollama_llm)\n",
    "print(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 10 is greater than number of elements in index 8, updating n_results = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response: Batterien gehören nicht in die Blaue Tonne. Um sie ordnungsgemäß zu entsorgen, sollte man sie an Sammelstellen wie Kommunale Sammelstellen, Wertstoffhöfe, Schadstoffmobile oder Freiwillige Sammelstellen abgeben. Die genauen Informationen zur Batteriesammlung erhält man bei der kommunalen Abfallberatung.\n"
     ]
    }
   ],
   "source": [
    "# Test RAG\n",
    "import pytest\n",
    "\n",
    "# Beispiel-Implementierung des Testfalls\n",
    "def test_rag_system():\n",
    "    query5 = \"Wie entsorge ich Batterien?\"\n",
    "\n",
    "    # Durchführung der Schritte in deiner RAG-Pipeline\n",
    "    keywords5 = extract_keywords_from_query(query5)\n",
    "    hybrid_results_v5 = hybrid_search_v5(chroma_client, query5, keywords5, embedding_model)\n",
    "    filtered_context_transformers = build_filtered_context_transformers(hybrid_results_v5, top_n=3)\n",
    "    llm_response = ask_llm_with_ollama(query5, filtered_context_transformers, ollama_llm)\n",
    "    \n",
    "    # Ausgabe der generierten Antwort\n",
    "    print(\"LLM Response:\", llm_response)\n",
    "    \n",
    "    # Überprüfen, ob die Antwort relevante Informationen enthält\n",
    "    assert \"Batterien\" in llm_response, \"Die Antwort sollte Informationen zur Entsorgung von Batterien enthalten.\"\n",
    "\n",
    "# Ausführen des Tests in einem Jupyter-Notebook\n",
    "test_rag_system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Keywords: ['entsorge', 'glasflaschen']\n",
      "Test 'test_extract_keywords' erfolgreich.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 10 is greater than number of elements in index 8, updating n_results = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 'test_hybrid_search' erfolgreich.\n",
      "Test 'test_llm_response' erfolgreich.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 10 is greater than number of elements in index 8, updating n_results = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 'test_full_pipeline_glasflaschen' erfolgreich.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 10 is greater than number of elements in index 8, updating n_results = 8\n",
      "Number of requested results 10 is greater than number of elements in index 8, updating n_results = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 'test_full_pipeline_batterien' erfolgreich.\n",
      "Test 'test_empty_query' erfolgreich.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 10 is greater than number of elements in index 8, updating n_results = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 'test_unknown_query' fehlgeschlagen: Die Antwort sollte keine falschen Informationen zu unbekannten Themen enthalten.\n",
      "Test 'test_response_quality_glasflaschen' fehlgeschlagen: Die Qualität der Antwort sollte hoch sein.\n"
     ]
    }
   ],
   "source": [
    "# Importiere benötigte Module\n",
    "import pytest\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "\n",
    "# Unit-Tests\n",
    "def test_extract_keywords():\n",
    "    try:\n",
    "        query = \"Wie entsorge ich Glasflaschen?\"\n",
    "        keywords = extract_keywords_from_query(query)\n",
    "        print(\"Extracted Keywords:\", keywords)\n",
    "        assert \"glasflaschen\" in keywords, \"Schlüsselwort-Extraktion sollte 'glasflaschen' enthalten.\"\n",
    "        print(\"Test 'test_extract_keywords' erfolgreich.\")\n",
    "    except AssertionError as e:\n",
    "        print(f\"Test 'test_extract_keywords' fehlgeschlagen: {e}\")\n",
    "\n",
    "def test_hybrid_search():\n",
    "    try:\n",
    "        query = \"Wie entsorge ich Glasflaschen?\"\n",
    "        keywords = extract_keywords_from_query(query)\n",
    "        results = hybrid_search_v5(chroma_client, query, keywords, embedding_model)\n",
    "        assert len(results) > 0, \"Hybrid-Suche sollte Ergebnisse liefern.\"\n",
    "        print(\"Test 'test_hybrid_search' erfolgreich.\")\n",
    "    except AssertionError as e:\n",
    "        print(f\"Test 'test_hybrid_search' fehlgeschlagen: {e}\")\n",
    "\n",
    "def test_llm_response():\n",
    "    try:\n",
    "        query = \"Wie entsorge ich Glasflaschen?\"\n",
    "        context = \"Glasflaschen sollten in den Glascontainer entsorgt werden.\"\n",
    "        response = ask_llm_with_ollama(query, context, ollama_llm)\n",
    "        assert \"Glasflaschen\" in response, \"LLM-Antwort sollte Informationen zu Glasflaschen enthalten.\"\n",
    "        print(\"Test 'test_llm_response' erfolgreich.\")\n",
    "    except AssertionError as e:\n",
    "        print(f\"Test 'test_llm_response' fehlgeschlagen: {e}\")\n",
    "\n",
    "# Integrationstests\n",
    "def test_full_pipeline_glasflaschen():\n",
    "    try:\n",
    "        query = \"Wie entsorge ich Glasflaschen?\"\n",
    "        keywords = extract_keywords_from_query(query)\n",
    "        hybrid_results = hybrid_search_v5(chroma_client, query, keywords, embedding_model)\n",
    "        filtered_context = build_filtered_context_transformers(hybrid_results, top_n=3)\n",
    "        response = ask_llm_with_ollama(query, filtered_context, ollama_llm)\n",
    "        assert \"Glasflaschen\" in response, \"Die Antwort sollte Informationen zur Entsorgung von Glasflaschen enthalten.\"\n",
    "        print(\"Test 'test_full_pipeline_glasflaschen' erfolgreich.\")\n",
    "    except AssertionError as e:\n",
    "        print(f\"Test 'test_full_pipeline_glasflaschen' fehlgeschlagen: {e}\")\n",
    "\n",
    "def test_full_pipeline_batterien():\n",
    "    try:\n",
    "        query = \"Wie entsorge ich Batterien?\"\n",
    "        keywords = extract_keywords_from_query(query)\n",
    "        hybrid_results = hybrid_search_v5(chroma_client, query, keywords, embedding_model)\n",
    "        filtered_context = build_filtered_context_transformers(hybrid_results, top_n=3)\n",
    "        response = ask_llm_with_ollama(query, filtered_context, ollama_llm)\n",
    "        assert \"Batterien\" in response, \"Die Antwort sollte Informationen zur Entsorgung von Batterien enthalten.\"\n",
    "        print(\"Test 'test_full_pipeline_batterien' erfolgreich.\")\n",
    "    except AssertionError as e:\n",
    "        print(f\"Test 'test_full_pipeline_batterien' fehlgeschlagen: {e}\")\n",
    "\n",
    "# Edge-Case-Tests\n",
    "def test_empty_query():\n",
    "    try:\n",
    "        query = \"\"\n",
    "        keywords = extract_keywords_from_query(query)\n",
    "        assert keywords == [], \"Schlüsselwort-Extraktion sollte eine leere Liste für leere Anfragen liefern.\"\n",
    "        print(\"Test 'test_empty_query' erfolgreich.\")\n",
    "    except AssertionError as e:\n",
    "        print(f\"Test 'test_empty_query' fehlgeschlagen: {e}\")\n",
    "\n",
    "def test_unknown_query():\n",
    "    try:\n",
    "        query = \"Wie entsorge ich Raumschiffmotoren?\"\n",
    "        keywords = extract_keywords_from_query(query)\n",
    "        hybrid_results = hybrid_search_v5(chroma_client, query, keywords, embedding_model)\n",
    "        filtered_context = build_filtered_context_transformers(hybrid_results, top_n=3)\n",
    "        response = ask_llm_with_ollama(query, filtered_context, ollama_llm)\n",
    "        assert \"Raumschiff\" not in response, \"Die Antwort sollte keine falschen Informationen zu unbekannten Themen enthalten.\"\n",
    "        print(\"Test 'test_unknown_query' erfolgreich.\")\n",
    "    except AssertionError as e:\n",
    "        print(f\"Test 'test_unknown_query' fehlgeschlagen: {e}\")\n",
    "\n",
    "def test_response_quality_glasflaschen():\n",
    "    try:\n",
    "        query = \"Wie entsorge ich Glasflaschen?\"\n",
    "        expected_response = \"Glasflaschen sollten in den Glascontainer entsorgt werden.\"\n",
    "        \n",
    "        keywords = extract_keywords_from_query(query)\n",
    "        hybrid_results = hybrid_search_v5(chroma_client, query, keywords, embedding_model)\n",
    "        filtered_context = build_filtered_context_transformers(hybrid_results, top_n=3)\n",
    "        response = ask_llm_with_ollama(query, filtered_context, ollama_llm)\n",
    "        \n",
    "        scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "        scores = scorer.score(expected_response, response)\n",
    "        rouge_score = scores['rougeL'].fmeasure\n",
    "        \n",
    "        assert rouge_score > 0.7, \"Die Qualität der Antwort sollte hoch sein.\"\n",
    "        print(\"Test 'test_response_quality_glasflaschen' erfolgreich.\")\n",
    "    except AssertionError as e:\n",
    "        print(f\"Test 'test_response_quality_glasflaschen' fehlgeschlagen: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ein Fehler ist im Test 'test_response_quality_glasflaschen' aufgetreten: {e}\")\n",
    "\n",
    "\n",
    "# Ausführen aller Tests in einem Jupyter-Notebook\n",
    "def run_all_tests():\n",
    "    test_extract_keywords()\n",
    "    test_hybrid_search()\n",
    "    test_llm_response()\n",
    "    test_full_pipeline_glasflaschen()\n",
    "    test_full_pipeline_batterien()\n",
    "    test_empty_query()\n",
    "    test_unknown_query()\n",
    "    test_response_quality_glasflaschen()\n",
    "\n",
    "run_all_tests()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predictiv_chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
